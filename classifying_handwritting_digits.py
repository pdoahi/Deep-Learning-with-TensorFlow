# -*- coding: utf-8 -*-
"""Classifying_Handwritting_Digits.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_fRjHiqq6y7DjoU0MaK8bE_c1_z8-zkG

**Assignment 1** :
  Classifying Handwritting digits

# Loading and Preparing the data
"""

#Import Libraries
import tensorflow as tf
from tensorflow import keras

# Loading MNIST dataset
mnist = keras.datasets.mnist 
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

# Data Exploration
print(X_train.shape)
print(X_test.shape)

# X_train is flattened to change the dimension
RESHAPED = 784 # 28x28 = 784 neurons
X_train = X_train.reshape(60000, RESHAPED) 
X_test = X_test.reshape(10000, RESHAPED) 

# Convert data type to float 
X_train = X_train.astype('float32')
X_test = X_test.astype('float32') 

# Normalizing the input to be within the range [0,1]
X_train /= 255
#intensity of each pixel is divided by 255, the maximum intensity value
X_test /= 255
print(X_train.shape[0], 'train samples') 
print(X_test.shape[0], 'test samples') 

# One-hot representation of the labels.
Y_train = tf.keras.utils.to_categorical(Y_train, 10) # 10 is the number of unique label
Y_test = tf.keras.utils.to_categorical(Y_test, 10)

"""#**Designing the model**

Model 0: Single Layer Perceptron
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras import Sequential

#No hidden layer
model_0 = keras.models.Sequential()

# Input arrays of shape (*, 784)
model_0.add(keras.layers.Dense(10, activation='softmax')) 

# Compiling the model.
model_0.compile(optimizer='SGD', 
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Training the model. 
training = model_0.fit(X_train, Y_train, batch_size=64, epochs=70, validation_split=0.2)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
# list all data in training
print(training.history.keys())
# summarize training for accuracy
plt.plot(training.history['accuracy'])
plt.plot(training.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize traning for loss
plt.plot(training.history['loss'])
plt.plot(training.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#evaluate the model
test_loss, test_acc = model_0.evaluate(X_test, Y_test)
print('Test accuracy:', test_acc)

""".

Model 1 : Multi Layer Perceptron
"""

# Building model with 2 hidden layers
from tensorflow.keras.layers import Dropout

model_1 = keras.models.Sequential()
model_1.add(keras.layers.Dense(200, activation='relu'))
model_1.add(Dropout(0.3))

model_1.add(keras.layers.Dense(100, activation='relu')) 
model_1.add(Dropout(0.3))

model_1.add(keras.layers.Dense(10, activation='softmax'))  

# Compiling the model.
model_1.compile(optimizer='SGD', 
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Training the model. 
training2 = model_1.fit(X_train, Y_train, batch_size=64, epochs=100, validation_split=0.2)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
# list all data in training
print(training2.history.keys())
# summarize training for accuracy
plt.plot(training2.history['accuracy'])
plt.plot(training2.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize traning for loss
plt.plot(training2.history['loss'])
plt.plot(training2.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#evaluate the model
test_loss, test_acc = model_1.evaluate(X_test, Y_test)
print('Test accuracy:', test_acc)

